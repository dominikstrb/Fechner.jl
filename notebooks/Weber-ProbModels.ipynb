{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling Interact [c601a237-2ae4-5e1e-952c-7a85b0c7eef1]\n",
      "└ @ Base loading.jl:1273\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>\n",
       "// Immediately-invoked-function-expression to avoid global variables.\n",
       "(function() {\n",
       "    var warning_div = document.getElementById(\"webio-warning-3724754045981508010\");\n",
       "    var hide = function () {\n",
       "        var script = document.getElementById(\"webio-setup-10028445350747437537\");\n",
       "        var parent = script && script.parentElement;\n",
       "        var grandparent = parent && parent.parentElement;\n",
       "        if (grandparent) {\n",
       "            grandparent.style.display = \"none\";\n",
       "        }\n",
       "        warning_div.style.display = \"none\";\n",
       "    };\n",
       "    if (typeof Jupyter !== \"undefined\") {\n",
       "        console.log(\"WebIO detected Jupyter notebook environment.\");\n",
       "        // Jupyter notebook.\n",
       "        var extensions = (\n",
       "            Jupyter\n",
       "            && Jupyter.notebook.config.data\n",
       "            && Jupyter.notebook.config.data.load_extensions\n",
       "        );\n",
       "        if (extensions && extensions[\"webio-jupyter-notebook\"]) {\n",
       "            // Extension already loaded.\n",
       "            console.log(\"Jupyter WebIO nbextension detected; not loading ad-hoc.\");\n",
       "            hide();\n",
       "            return;\n",
       "        }\n",
       "    } else if (window.location.pathname.includes(\"/lab\")) {\n",
       "        // Guessing JupyterLa\n",
       "        console.log(\"Jupyter Lab detected; make sure the @webio/jupyter-lab-provider labextension is installed.\");\n",
       "        hide();\n",
       "        return;\n",
       "    }\n",
       "})();\n",
       "\n",
       "</script>\n",
       "<p\n",
       "    id=\"webio-warning-3724754045981508010\"\n",
       "    class=\"output_text output_stderr\"\n",
       "    style=\"padding: 1em; font-weight: bold;\"\n",
       ">\n",
       "    Unable to load WebIO. Please make sure WebIO works for your Jupyter client.\n",
       "    For troubleshooting, please see <a href=\"https://juliagizmos.github.io/WebIO.jl/latest/providers/ijulia/\">\n",
       "    the WebIO/IJulia documentation</a>.\n",
       "    <!-- TODO: link to installation docs. -->\n",
       "</p>\n"
      ],
      "text/plain": [
       "HTML{String}(\"<script>\\n// Immediately-invoked-function-expression to avoid global variables.\\n(function() {\\n    var warning_div = document.getElementById(\\\"webio-warning-3724754045981508010\\\");\\n    var hide = function () {\\n        var script = document.getElementById(\\\"webio-setup-10028445350747437537\\\");\\n        var parent = script && script.parentElement;\\n        var grandparent = parent && parent.parentElement;\\n        if (grandparent) {\\n            grandparent.style.display = \\\"none\\\";\\n        }\\n        warning_div.style.display = \\\"none\\\";\\n    };\\n    if (typeof Jupyter !== \\\"undefined\\\") {\\n        console.log(\\\"WebIO detected Jupyter notebook environment.\\\");\\n        // Jupyter notebook.\\n        var extensions = (\\n            Jupyter\\n            && Jupyter.notebook.config.data\\n            && Jupyter.notebook.config.data.load_extensions\\n        );\\n        if (extensions && extensions[\\\"webio-jupyter-notebook\\\"]) {\\n            // Extension already loaded.\\n            console.log(\\\"Jupyter WebIO nbextension detected; not loading ad-hoc.\\\");\\n            hide();\\n            return;\\n        }\\n    } else if (window.location.pathname.includes(\\\"/lab\\\")) {\\n        // Guessing JupyterLa\\n        console.log(\\\"Jupyter Lab detected; make sure the @webio/jupyter-lab-provider labextension is installed.\\\");\\n        hide();\\n        return;\\n    }\\n})();\\n\\n</script>\\n<p\\n    id=\\\"webio-warning-3724754045981508010\\\"\\n    class=\\\"output_text output_stderr\\\"\\n    style=\\\"padding: 1em; font-weight: bold;\\\"\\n>\\n    Unable to load WebIO. Please make sure WebIO works for your Jupyter client.\\n    For troubleshooting, please see <a href=\\\"https://juliagizmos.github.io/WebIO.jl/latest/providers/ijulia/\\\">\\n    the WebIO/IJulia documentation</a>.\\n    <!-- TODO: link to installation docs. -->\\n</p>\\n\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Distributions\n",
    "using Plots\n",
    "using LaTeXStrings\n",
    "using Interact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weber's Law\n",
    "Weber's law (WL), initially reported in 1834 (Weber, 1834) and later formalized by Fechner (1860), states that the just-noticeable-difference (JND) between two stimuli grows linearly with the stimulus magnitude:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\Delta_{\\theta} = w \\cdot \\theta,\n",
    "\\end{align}\n",
    "\n",
    "where $w = \\frac{ \\Delta_{\\theta}}{\\theta}$ is also known as the Weber fraction (WF).\n",
    "\n",
    "It is important to note that the JND is a statement about a discrimination task. The JND $ \\Delta_{\\theta}$ is the difference in two stimuli $\\theta_1 = \\theta + \\frac{ \\Delta_{\\theta}}{2}$ and $\\theta_2 = \\theta - \\frac{ \\Delta_{\\theta}}{2}$ around a value $\\theta$ that is noticed in a fraction $p$ of repetitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Models of Perception\n",
    "In probabilistic models of perception (Ma, Kording & Goldreich, unpublished), one typically assumes that observers have access to a noisy measurement $m$ of a presented stimulus $\\theta$ with some probability distribution $p(m | \\theta)$, which is often called the measurement (or noise) distribution. The observer's goal is then to compute an estimate of the presented stimulus $\\hat \\theta(m)$. Critically, while the observer's noisy measurement is a random variable, the estimate is a completely deterministic function of the measurement $m$. As an experimenter, we do not know the noisy internal measurement of an observer in any given trial. We can, however, compute the distribution of the estimates given a particular stimulus $p(\\hat\\theta | \\theta)$.\n",
    "\n",
    "JNDs are usually expressed in probabilistic model of perception in the following way. For a small $\\Delta_{\\theta}$, we can locally assume that the measurement distributions are Gaussian with equal variance $\\sigma_\\theta^2$. Then, in a 2-AFC discrimination task, the distribution of the difference of the measurements is Gaussian with variance $\\frac{\\sigma_\\theta^2}{2}$ and thus the probability of estimating that $\\hat\\theta_1 > \\hat\\theta_2$ is given by a cumulative Gaussian with mean $\\theta_1 - \\theta_2$ and standard deviation $\\frac{\\sigma_\\theta}{\\sqrt{2}}$:\n",
    "\n",
    "\\begin{align}\n",
    "    p(\\hat\\theta_1 - \\hat\\theta_2 | \\theta_1, \\theta_2) = \\Phi(\\theta_1 - \\theta_2 | 0, \\frac{\\sigma_\\theta}{\\sqrt{2}}) = \\Phi(\\Delta_{\\theta} | 0, \\frac{\\sigma_\\theta}{\\sqrt{2}})\n",
    "\\end{align}\n",
    "\n",
    "Thus, for a given discrimination probability $p$, we have $\\Delta_{\\theta} = \\Phi^{-1}(p | 0, \\frac{\\sigma_\\theta}{\\sqrt{2}})$, from which follows that\n",
    "\n",
    "\\begin{align}\n",
    "    \\Delta_{\\theta} \\propto \\frac{\\sigma_\\theta}{\\sqrt{2}},\n",
    "\\end{align}\n",
    "\n",
    "where the proportionality constant depends on $p$. The $\\sqrt{2}$ factor results from the 2-AFC task and would be different for tasks with more choice alternatives. This means that the standard deviation of the assumed measurement distribution is the same as the discriminability (up to a constant factor).\n",
    "\n",
    "If we now assume that WL holds (or better, if we have measured that it holds), we can interpret this as a statement about the standard deviation of the measurement distribution. Particularly, it means that we get a linear scaling of the standard deviation with the stimulus magnitude\n",
    "\n",
    "\\begin{align}\n",
    "    \\sigma_\\theta = w \\cdot \\theta.\n",
    "\\end{align}\n",
    "\n",
    "This linear scaling of standard deviation is often used in combination with Gaussian measurement distributions in probabilistic observer models (e.g. Jazayri & Shadlen, 2010; Cicchini et al, 2012). \n",
    "\n",
    "Keep in mind that we have not used the assumption of linearly scaled standard deviation in measuring the discrimination thresholds, but have assumed locally that the standard deviation is constant. For measurements of $\\Delta_{\\theta}$ at different reference stimuli, we can check whether WL holds.\n",
    "\n",
    "An alternative to Gaussian noise with linearly scaled standard deviations is a logarithmic encoding of the stimuli. Here, the assumption is that $\\psi = \\ln{\\theta}$ is Gaussian distributed with equal variance $\\sigma^2_{\\text{log}}$ everywhere. In this case, the measurements on the physical stimulus dimension follow a log-normal distribution:\n",
    "\n",
    "\\begin{align}\n",
    "    p(m | \\theta) = \\text{Lognormal}(m | \\ln{\\theta}, \\sigma_{\\text{log}}).\n",
    "\\end{align}\n",
    "\n",
    "Since the standard deviation of the log-normal distribution is $\\sqrt{[e^{\\sigma_{\\text{log}}^2} - 1] \\, e^{2 \\ln \\theta + \\sigma_{\\text{log}}^2}}$, it can be written as a linear function of the stimulus magnitude: $\\sigma_\\theta = c \\cdot \\theta$ with $c = \\sqrt{[e^{\\sigma_{\\text{log}}^2} - 1]\\,e^{\\sigma_{\\text{log}}^2}}$.\n",
    "\n",
    "In probabilistic models of perception, the log-normal assumption is a popular way of dealing with magnitude variables, for which WL scaling of standard deviation is a good approximatition (e.g. Petzschner & Glasauer, 2011; Battaglia et al., 2011). This assumption makes computing estimates more convenient and avoids some problems of the Gaussian case, as we will see in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-likelihood estimates\n",
    "The observer does not have access to the actually presented stimulus $\\theta_0$. If we assume that they still have knowledge about their own noise distribution, they can make use of the likelihood function. The likelihood function is the probability of the observed measurement $m$ as a function of the \"true\" stimulus $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda(\\theta; m) = p(m | \\theta).\n",
    "\\end{align}\n",
    "\n",
    "An estimate $\\hat\\theta(m)$ is usually computed using this likelihood function (and possibly a prior distribution and a cost function). Here, we focus first on the simplest case, a maximum likelihood (ML) estimate $\\hat\\theta(m) = \\text{argmax}_\\theta \\lambda(\\theta; m)$ and compare the normal with scaled standard deviation and log-normal models.\n",
    "\n",
    "\\subsection{Normal distribution with scaled variance}\n",
    "For the normal distribution with scaled variance, the likelihood function is\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda(\\theta; m) = p(m | \\theta) = \\frac{1}{w\\theta \\sqrt{2\\pi}} \\exp\\left\\{\\frac{-(m - \\theta)^2}{2(w\\theta)^2}\\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "The ML estimate can then be computed (see Jazayeri & Shadlen, 2010) as\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat\\theta_{\\text{MLE}} = \\text{argmax}_\\theta \\lambda(\\theta; m) = m \\left[\\frac{\\sqrt{1 + 4w^2} - 1}{2 w^2} \\right].\n",
    "\\end{align}\n",
    "\n",
    "% TODO: add plot propconst = f(w)\n",
    "\n",
    "The proportionality constant is smaller than 1, resulting in a systematic underestimation of the true $\\theta$, which becomes more severe the higher the WF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
